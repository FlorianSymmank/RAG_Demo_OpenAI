[
    {
        "filename": "Belegung - Tabellenblatt1.pdf",
        "text": "Montag\n\nDienstag\n\nMittwoch\n\nDonnerstag\n\nFreitag\n\nSamstag\n\nSonntag\n\nDatenschutz SL\n\nProjektstudium Hacken\n\nDatenschutz \u00dc\n\nMathe 2 \n\nMM Ausgew\u00e4hlte Kapitel SL\n\nSpezielle Anwendung \u00dc HPC\n\nMath 2 \u00dc\n\nSpezielle Anwendungen SL HPC MM Ausgew\u00e4hlte Kapitel \u00dc\n\nMA Ausgew\u00e4hlte Kapitel SL\n\nKomponenten \u00dc\n\nKomponenten SL\n\nMA Ausw\u00e4hlte Kapitel \u00dc\n\n08:00\n\n08:15:00\n\n08:30\n\n08:45:00\n\n09:00\n\n09:15:00\n\n09:30\n\n09:45:00\n\n10:00\n\n10:15:00\n\n10:30\n\n10:45:00\n\n11:00\n\n11:15\n\n11:30:00\n\n11:45\n\n12:00:00\n\n12:15\n\n12:30:00\n\n12:45\n\n13:00:00\n\n13:15\n\n13:30:00\n\n13:45\n\n14:00:00\n\n14:15\n\n14:30\n\n14:45:00\n\n15:00\n\n15:15:00\n\n15:30\n\n15:45:00\n\n16:00\n\n16:15:00\n\n16:30\n\n16:45:00\n\n17:00\n\n17:15:00\n\n17:30\n\n17:45\n\n18:00:00\n\n18:15\n\n18:30:00\n\n18:45\n\n19:00:00\n\n19:15\n\n19:30:00\n\n19:45\n\n20:00:00\n\n20:15\n\n20:30:00\n\n20:45\n\n21:00\n\n21:15:00\n\n21:30\n\n21:45:00\n\n22:00\n\n\f",
        "pages_description": [
            "This is a weekly schedule detailing various activities or classes from Monday to Sunday. The schedule is organized by time slots starting from 08:00 to 22:00. Here is a breakdown of the activities:\n\n**Monday:**\n- 09:00 - 10:30: Datenschutz SL\n- 12:00 - 13:30: Datenschutz \u00dc\n- 17:30 - 19:00: MA Ausgew\u00e4hlte Kapitel SL\n\n**Tuesday:**\n- 09:00 - 10:30: Projektstudium Hacken\n- 12:00 - 13:30: Mathe 2\n- 15:00 - 16:30: Math 2 \u00dc\n- 18:00 - 19:30: Komponenten \u00dc\n- 19:30 - 21:00: MA Ausgew\u00e4hlte Kapitel \u00dc\n\n**Wednesday:**\n- 12:00 - 13:30: Mathe 2\n\n**Thursday:**\n- 14:00 - 15:30: Spezielle Anwendung \u00dc HPC\n- 16:00 - 17:30: Spezielle Anwendungen SL HPC\n\n**Friday:**\n- 08:00 - 09:30: MM Ausgew\u00e4hlte Kapitel SL\n- 14:00 - 15:30: MM Ausgew\u00e4hlte Kapitel \u00dc\n- 16:00 - 17:30: Komponenten SL\n\n**Saturday and Sunday:**\n- No activities"
        ]
    },
    {
        "filename": "Lebenslauf2023.pdf",
        "text": "Florian Symmank\n\nRennbahnstra\u00dfe 127\n13086 Berlin\nFlorianSymmank@gmx.de\n0176/48675636\nGeboren am 14.12.1997\n\nF\u00c4HIGKEITEN\n\nSprachen:\n\n\u25cf Deutsch - Muttersprache\n\u25cf Englisch - sehr gut\n\nBERUFSERFAHRUNG\n\nProgrammiersprachen:\n\n\u25cf C#, Java, Python, JS, SQL\n\nAtos Information Technology GmbH, Berlin - Werkstudent Software Solution Engineer\n\nApril 2022 - M\u00e4rz 2023\n\n\u25cf Microsoft Power Apps - Weiterentwicklung einer Netzwerkmanagementanwendung\n\u25cf Webentwicklung - Kon zep ti o nie rung und Entwicklung einer BLE Location Service Anwendung\n\nGVXL Software GmbH, Berlin - IT-Systemkaufmann\n\nAugust 2019 - November 2020\n\n\u25cf Entwicklung einer Gerichtsvollziehersoftware\n\u25cf Dokumentation\n\u25cf Fehleranalyse beim Kunden und Kundensupport\n\u25cf Kundenbetreuung\n\nGVXL Software GmbH, Berlin - Ausbildung zum IT-Systemkaufmann\n\nSeptember 2016 - August 2019\n\n\u25cf praktischer Teil der Fachhochschulreife\n\u25cf Softwareentwicklung\n\u25cf kaufm\u00e4nnische T\u00e4tigkeiten\n\nBILDUNGSWEG\n\nHochschule f\u00fcr Technik und Wirtschaft, Berlin - Bachelor of Science in Angewandte Informatik\n\nNovember 2020 \u2013 heute\n\n\u25cf Embedded Systems: Zepyhr ROTS Temperatursensor und Display\n\u25cf Computergrafik: OpenGL 3D-Diagramme\n\u25cf Gestaltung und Entwicklung von Multimediasystemen: Unity VR-Galerie\n\u25cf Mobile Anwendungen: Android App Conway\u2019s Game of Life\n\nKurt-Tucholsky-Oberschule, Berlin - Abitur\n\nAugust 2014 \u2013 Juli 2016\n\n\u25cf schulischer Teil der Fachhochschulreife\n\n\f",
        "pages_description": [
            "Florian Symmannk\n\n**F\u00e4higkeiten**\n\n- **Sprachen:**\n  - Deutsch - Muttersprache\n  - Englisch - sehr gut\n\n- **Programmiersprachen:**\n  - C#, Java, Python, JS, SQL\n\n**Berufserfahrung**\n\n- **Atos Information Technology GmbH, Berlin - Werkstudent Software Solution Engineer**\n  - April 2022 - M\u00e4rz 2023\n    - Microsoft Power Apps - Weiterentwicklung einer Netzwerkmanagementanwendung\n    - Webentwicklung - Konzeptionierung und Entwicklung einer BLE Location Service Anwendung\n\n- **GVXL Software GmbH, Berlin - IT-Systemkaufmann**\n  - August 2019 - November 2020\n    - Entwicklung einer Gerichtsvollziehersoftware\n    - Dokumentation\n    - Fehleranalyse beim Kunden und Kundensupport\n    - Kundenbetreuung\n\n- **GVXL Software GmbH, Berlin - Ausbildung zum IT-Systemkaufmann**\n  - September 2016 - August 2019\n    - praktischer Teil der Fachhochschulreife\n    - Softwareentwicklung\n    - kaufm\u00e4nnische T\u00e4tigkeiten\n\n**Bildungsweg**\n\n- **Hochschule f\u00fcr Technik und Wirtschaft, Berlin - Bachelor of Science in Angewandte Informatik**\n  - November 2020 \u2013 heute\n    - Embedded Systems: Zephyr ROTS Temperatursensor und Display\n    - Computergrafik: OpenGL 3D"
        ]
    },
    {
        "filename": "NextDocument(3).pdf",
        "text": "\f",
        "pages_description": [
            "Erkl\u00e4rung\n\nder externen Einrichtung \u00fcber die Gew\u00e4hrleistung der pr\u00fcfungsordnungskonformen Durchf\u00fchrung einer Bachelorarbeit im Studiengang \"Angewandte Informatik\" der HTW Berlin\n\nAngaben zur externen Einrichtung\nName: Fraunhofer-Institut f\u00fcr Offene Kommunikationssysteme FOKUS\nAdresse: Kaiserin-Augusta-Allee 31, 10589 Berlin\nAnsprechpartner: Carsten Tittel\nE-Mail-Adresse: carsten.tittel@fokus.fraunhofer.de\n\nErkl\u00e4rung\nHiermit erkl\u00e4ren wir, dass in unserer Einrichtung die Voraussetzungen zur Anfertigung einer Bachelorarbeit mit dem Thema: \"Evaluierung von Retrieval-Augmented Generation und Fine-Tuning in der Wissensrepr\u00e4sentation: Ein Vergleich mit gro\u00dfen Sprachmodellen\" gegeben sind.\n\nDas Thema ist vom Tagesgesch\u00e4ft abgegrenzt. Die Themenbearbeiterin bzw. der Themenbearbeiter ist nicht in das Tagesgesch\u00e4ft eingebunden. Die Themenbearbeitung kann w\u00e4hrend des Bearbeitungszeitraumes in Vollzeit erfolgen. Ein entsprechender Arbeitsplatz ist vorhanden.\n\nOrt, Datum\n\nUnterschrift des Ansprechpartners"
        ]
    },
    {
        "filename": "trETFnAkpLYv2299Wr3R9H5DIQZvX4Irvld5YhWH(2).pdf",
        "text": "EVALUATION METRICS FOR LANGUAGE MODELS\n\nStanley Chen, Douglas Beeferman, Ronald Rosenfeld\n\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213\nsfc,dougb,roni (cid:2) @cs.cmu.edu\n\nABSTRACT\n\nThe most widely-used evaluation metric for language models for\nspeech recognition is the perplexity of test data. While perplex-\nities can be calculated ef\ufb01ciently and without access to a speech\nrecognizer, they often do not correlate well with speech recognition\nword-error rates. In this research, we attempt to \ufb01nd a measure that\nlike perplexity is easily calculated but which better predicts speech\nrecognition performance.\n\nWe investigate two approaches; \ufb01rst, we attempt to extend perplex-\nity by using similar measures that utilize information about language\nmodels that perplexity ignores. Second, we attempt to imitate the\nword-error calculation without using a speech recognizer by arti\ufb01-\ncially generating speech recognition lattices. To test our new metrics,\nwe have built over thirty varied language models. We \ufb01nd that per-\nplexity correlates with word-error rate remarkably well when only\n-gram models trained on in-domain data. When con-\nconsidering\nsidering other types of models, our novel metrics are superior to\nperplexity for predicting speech recognition performance. How-\never, we conclude that none of these measures predict word-error\nrate suf\ufb01ciently accurately to be effective tools for language model\nevaluation in speech recognition.\n\n1. INTRODUCTION\n\nIn the literature, two primary metrics are used to estimate the perfor-\nmance of language models in speech recognition systems. First, they\nare evaluated by the word-error rate (WER) yielded when placed in\na speech recognition system. Second, and more commonly, they\nare evaluated through their perplexity on test data, an information-\ntheoretic assessment of their predictive power.\n\nWhile word-error rate is currently the most popular method for rating\nspeech recognition performance, it is computationally expensive to\ncalculate. Furthermore, its calculation generally requires access\nto the innards of a speech recognition system, few of which are\npublically available. Finally, word-error rate is speech-recognizer-\ndependent, which makes it dif\ufb01cult for different research sites to\ncompare language models with this measure.\n\nPerplexity, on the other hand, can be computed trivially and\nin isolation;\nof a language model\n\nthe perplexity PP\n\n(cid:4)(cid:6)(cid:5)(cid:8)(cid:7)(cid:10)(cid:9)\n\nThis work was supported by the National Security Agency under grants\nMDA904-96-1-0113 and MDA904-97-1-0006 and by the DARPA AASERT\naward DAAH04-95-1-0475. The views and conclusions contained in this\ndocument are those of the authors and should not be interpreted as represent-\ning the of\ufb01cial policies, either expressed or implied, of the U.S. government.\n\n(cid:5)(cid:8)(cid:7)(cid:12)(cid:4)\n\n(cid:13)(cid:15)(cid:14)\n\n(cid:16)(cid:8)(cid:9)\n\n(cid:17)(cid:19)(cid:18)\n\n(cid:13)(cid:25)(cid:24)(cid:26)(cid:2)\n\nnext word\n\nhistory\n\non a test set\n\n(cid:27)(cid:28)(cid:4)(cid:6)(cid:5)\n\n(cid:9)(cid:29)(cid:18)\n\n(cid:30) (cid:31)\n\n!#\"\n\nPP\n\n1\n\n1\n\n!\u2019&\n\n(cid:20)(cid:22)(cid:21)(cid:22)(cid:21)(cid:22)(cid:21)(cid:23)(cid:20)\n\nis just\n\n1\n\n(1)\n\n(cid:4)$(cid:13)\n\n(cid:9)(cid:26)(\n\n%(cid:22)%(cid:22)%\n\n1\n\n1\n\n1\n\nor the inverse of the (geometric) average probability assigned to each\nword in the test set by the model. Perplexity is theoretically elegant\nas its logarithm is an upper bound on the number of bits per word\nexpected in compressing (in-domain) text employing the measured\nmodel. Unfortunately, while language models with lower perplexi-\nties tend to have lower word-error rates, there have been numerous\nexamples in the literature where language models providing a large\nimprovement in perplexity over a baseline model have yielded little\nor no improvement in word-error rate [1, 2]. In addition, perplexity\nis inapplicable to unnormalized language models (i.e., models that\nare not true probability distributions that sum to 1), and perplexity\nis not comparable between language models with different vocabu-\nlaries. In this research, we attempt to \ufb01nd a measure for evaluating\nlanguage models that is applicable to unnormalized models and that\npredicts word-error rate more accurately than perplexity but which,\nlike perplexity, is computationally inexpensive and can be computed\nseparately from a speech recognition system. We consider two dif-\nferent approaches to this task.\n\nOur \ufb01rst approach involves extending perplexity to utilize informa-\ntion that it previously ignores. As can be seen from equation (1),\nperplexity depends only on the probabilities assigned to actual text.\nHowever, word-error rate depends on the probabilities assigned to\nall transcriptions hypothesized by a speech recognizer; errors occur\nwhen an incorrect hypothesis has a higher score than the correct\nhypothesis. We consider metrics that harness this information.\n\nOur second approach involves an attempt to mimic the process of\ncalculating word-error rate through lattice rescoring, without actually\nusing a speech recognition system to construct lattices. Instead, we\narti\ufb01cially generate lattices and evaluate language models through\ntheir word-error rates on these arti\ufb01cial lattices.\n\nTo evaluate our novel language model measures, we have constructed\n-\nover thirty language models of varying types, including class\ngram[3, 4], trigger[5], and cache[6] language models. We \ufb01nd that\nperplexity correlates with word-error rate remarkably well when\nonly considering\n-gram models trained on in-domain data. When\nconsidering other types of models, our novel metrics are superior\nto perplexity for predicting speech recognition performance. How-\never, we conclude that none of these measures predict word-error\nrate suf\ufb01ciently accurately to be effective tools for language model\nevaluation in speech recognition.\n\n(cid:0)\n(cid:1)\n(cid:3)\n(cid:11)\n(cid:1)\n(cid:13)\n(cid:7)\n(cid:24)\n(cid:5)\n(cid:7)\n!\n(cid:14)\n(cid:13)\n(cid:13)\n)\n(cid:3)\n(cid:3)\n\f1.1. Previous Work\n\nIyer et al.[2] investigate the prediction of speech recognition perfor-\nmance for language models in the Switchboard domain, for trigram\nmodels built on differing amounts of in-domain and out-of-domain\ntraining data. Over the ten models they constructed, they \ufb01nd that\nperplexity predicts word-error rate well when only in-domain train-\ning data is used, but poorly when out-of-domain text is added. They\n\ufb01nd that trigram coverage, or the fraction of trigrams in the test data\npresent in the training data, is a better predictor of word-error rate\n-gram cov-\nthan perplexity. However, it is unclear how to extend\nerage to comparing other types of models, such as class models or\n-gram models of different order. In addition, this measure cannot\n\ndistinguish between different models trained on the same data.\n\nThey also present techniques for building a decision tree that predicts\nthe relative performance of two models on each word in a test set.\nUsing this decision tree, they are able to predict with high accuracy\nthe relative performance of pairs of trigram models. While this tech-\nnique seems promising, the features used to build the tree include\nlexical information such as part-of-speech information and the pho-\nnetic lengths of words. In this work, we would like to investigate\nwhat is possible with measures like perplexity that ignore detailed\nlexical information.\n\n1.2. Methodology\n\nIn this research, we investigate speech recognition performance in the\nBroadcast News domain. We generated narrow-beam lattices with\nthe Sphinx-III recognition system[7] using a trigram model trained\non 130M words of Broadcast News text; trigrams occurring only\nonce were excluded from the model. The word-error rates reported\nin this work were calculated by rescoring these lattices with the given\nlanguage model.\n\n-gram order, and\n\nWe created 35 language models, which we divided into two sets. Set\n-gram models built on Broadcast News training\nA contains only\ndata. The training set size, smoothing,\n-gram\ncutoffs were varied. Set B contains various kinds of models, includ-\n-gram class models, trigram models enhanced with a cache or\ning\n-gram models built on out-of-domain data, and models\ntriggers,\nthat are an interpolation of\n-gram models built on in-domain and\nout-of-domain data. In Table 1, we list the language models in each\nset. The held-out and test sets consist of 22,000 and 28,000 words,\nrespectively, of Broadcast News data.\n\n2. PERPLEXITY AND WORD-ERROR\nRATE\nIn Figure 1, we display a graph of word-error rate versus log per-\nplexity for each of the models in sets A and B. The linear correlation\nbetween word-error rate and log perplexity seems remarkably strong\n-gram models built\nfor the models in set A, which consists of only\non in-domain data, but less so for the models in set B, which is a\nmore disparate collection of models. This indicates that log perplex-\nity may be a good predictor of speech recognition performance when\nconsidering only particular types of models.\n\nIt seems somewhat surprising that log perplexity, which is measured\nin bits (recall the information theoretic interpretation of perplexity\nmentioned in Section 1), is correlated with the very different unit of\nword errors. To attempt to shed light on why these two apparently\nunrelated quantities are related, in Figure 2 we graph the relation-\n\nset A\n\ndata\n(wds)\n\nsmooth\nalg\n5M K-N[8]\n5M K-N\n5M K-N\n5M K-N\n5M K-N\n5M Katz[9]\n5M poor\n10M poor\n25M poor\n\n5M K-N (i)\n5M K-N (ii)\n1M K-N\n25M K-N\n130M K-N\n10M K-N\n25M K-N\n130M K-N\n\n1\n2\n3\n4\n5\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n2\n2\n\nset B\n\ndescription\n-gram model\nclass\n2\n-gram model\nclass\n3\n-gram model\nclass\n4\ntrigram model + cache 1\n3\ntrigram model + cache 2\n3\n3\ntrigram model, Katz\n3 Katz model + triggers 1\n3 Katz model + triggers 2\n2 AP news training data\n3 AP news training data\n4 AP news training data\n2\n3\n4\n3 AP and BN models mixed\n4 AP and BN models mixed\n3\n4\n\nSwitchboard (SWB) data\nSwitchboard data\nSwitchboard data\n\nSWB and BN models mixed\nSWB and BN models mixed\n\ncolumn describes\nTable 1: Language models in sets A and B. The\n-gram model (e.g., unigram or bigram). The data\nthe order of the\ncolumn describes the size of the training set used.\nIn set A, the\nmodel labeled (i) excludes all bigrams and trigrams with only one\ncount; the model labeled (ii) excludes all bigrams and trigrams with\ntwo or fewer counts. The abbreviation K-N stands for Kneser-Ney.\nThe smoothing method poor is an algorithm specially designed to\nIn set B, all models are trained on 5M words of\nperform poorly.\n-gram cutoffs, and are smoothed with Kneser-Ney\ndata, have no\nsmoothing except where otherwise speci\ufb01ed.\n\nship between the language model probability assigned to a word in\na test set and the chance that word is transcribed correctly in speech\nrecognition. The dotted lines represent curves for each of the in-\ndividual models in sets A and B. To generate each curve, we \ufb01rst\ncalculated the probability assigned by the given model to each word\nin our held-out set, and placed these words in logarithmically-spaced\nbuckets based on these probabilities. Then, from the corresponding\nspeech recognition run we used NIST\u2019s sclite software to mark each\nword in the held-out set as correct or incorrect. Finally, we calculated\nthe fraction of words in each bucket that are correct or incorrect.\n\nTo relate log perplexity and word-error rate, consider approximating\nthe curves in Figure 2 as a straight line, i.e.,\n\n(cid:5)(cid:8)(cid:7)*(cid:4)$(cid:13)\n\n(cid:9)(cid:28)+-,\n\n(cid:5)(cid:8)(cid:7)*(cid:4)$(cid:13).(cid:14)\n\n(cid:16)(cid:8)(cid:9)(cid:8)/0,\n\nis correct\n\n1 log\n\n2\n\n(cid:4)$(cid:13)(cid:15)(cid:14)\n\n(cid:16)2(cid:9)\n\nfor all models\nfor some constants\ndenotes the language model probability assigned to word\n\n2, where\n\n1 and\n\ngiven history\n\n. Then, for a test set\n\n(cid:17)3(cid:18)\n\n1\n\nexpected word accuracy is\n\n(cid:20)(cid:22)(cid:21)(cid:22)(cid:21)4(cid:21)(cid:22)(cid:20)\n\nby model\nthe\n\n(cid:13)(cid:25)(cid:24)5(cid:2)\n\n1\n\n!8\"\n\n1\n\n(cid:4)$(cid:13)\n\n(cid:9)9(cid:18)\n\nis correct\n\n1\n\n!#\"\n\n1 log\n\n(cid:4)$(cid:13)\n\n(cid:9);/<,\n\n2\n\n>(cid:25),\n\n1\n1 log PP\n\n(cid:27)(cid:28)(cid:4)(cid:6)(cid:5)\n\n(cid:9);/<,\n\n2\n\ni.e., the expected word accuracy is a linear function of the perplexity.\nIf we make the approximation that word-error rate is a linear function\n\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n(cid:3)\n1\n,\n,\n(cid:5)\n(cid:7)\n(cid:13)\n1\n(cid:16)\n(cid:1)\n(cid:13)\n6\n(cid:24)\n7\n(cid:5)\n(cid:7)\n!\n6\n(cid:24)\n7\n:\n,\n(cid:5)\n(cid:7)\n!\n(cid:14)\n(cid:16)\n!\n=\n(cid:18)\n(cid:7)\n\fe\nt\na\nr\n\nr\no\nr\nr\ne\n-\nd\nr\no\nw\n\ne\nt\na\nr\n\nr\no\nr\nr\ne\n-\nd\nr\no\nw\n\n52\n\n50\n\n48\n\n46\n\n44\n\n42\n\n40\n\n38\n\n36\n\n34\n\n45\n\n44\n\n43\n\n42\n\n41\n\n40\n\n39\n\n38\n\n37\n\n36\n\nset A\n\nt\nc\ne\nr\nr\no\nc\n\ng\nn\ni\ne\nb\n\nf\no\n\ny\nt\ni\nl\ni\nb\na\nb\no\nr\np\n\n0.9\n\n0.8\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n7\n\n7.5\n\n8\n\n8.5\nlog2 perplexity\n\n9\n\n9.5\n\n10\n\n1e-07\n\n1e-06\n\nset B\n\n1e-05\n\n0.0001\n\n0.001\n\n0.01\n\nlanguage model probability of word\n\n0.1\n\n1\n\nFigure 2: Probability of a word being correct in speech recognition\ngiven its language model probability. Each line represents one of the\nlanguage models in sets A and B.\n\nTo quantify the correlation between different metrics with word-error\n)\nrate, we calculate the linear correlation coef\ufb01cient (or Pearson\u2019s\nmeasuring the degree of linear correlation; the Spearman rank-order\ncorrelation coef\ufb01cient measuring how well the ranks of models lin-\nmeasuring how well the relative\nearly correlate; and Kendall\u2019s\nperformance of pairs of models is predicted. In Table 2, we display\nthese correlations for perplexity and M-ref versus word-error rate.\nFor set A, perplexity correlates with word-error rate better than mea-\nsure M-ref according to all three measures, while for set B measure\nM-ref is marginally better.\n\n3.2. Using Additional Information\n\nPerplexity and M-ref depend only on the probabilities of words in\nthe test set, which in speech recognition is simply the reference tran-\nscript. However, word-error rate depends also on the probabilities\nassigned to incorrect hypotheses; in particular, errors occur when an\nincorrect hypothesis outscores the correct hypothesis. For example,\nit seems intuitive that errors are more likely to occur when many\nincorrect words are assigned large language model probabilities.\n\n45\n\n44\n\n43\n\n42\n\n41\n\n40\n\n39\n\n38\n\n37\n\ne\nt\na\nr\n\nr\no\nr\nr\ne\n-\nd\nr\no\nw\n\n36\n0.255\n\n0.26\n\n0.265\n\n0.27\nmeasure M-ref\n\n0.275\n\n0.28\n\n0.285\n\nFigure 3: Word-error rate vs. measure M-ref on set B\n\n7.2\n\n7.4\n\n7.6\n\n7.8\n\n8\n\n8.2\n\n8.4\n\n8.6\n\n8.8\n\n9\n\nlog2 perplexity\n\nFigure 1: Word-error rate vs. log perplexity\n\nof word accuracy, then we have that word-error rate is also a linear\nfunction of perplexity.\n\nThis analysis, while very rough, does lend some insight as to why\nperplexity and word-error rate are at all related, and suggests where\nperplexity might be improved and where the perplexity-WER rela-\ntionship might break down. For example, it is clear that the linear\napproximation is poor for very low probabilities, where the proba-\nbility of correctness is predicted to be less than zero.\n\n3. EXTENDING PERPLEXITY\n3.1. Modeling the Relation between Language\n\nModel Probability and Word Accuracy\n\nOne natural technique to try given the analysis in Section 2 is to use\nthe functions displayed in Figure 2 to estimate word-error rate. That\nis, since our use of log perplexity to predict word-error rate can be\nviewed as being based on a hypothesis that these functions are linear,\nwe might do better with an empirically-estimated function. To im-\nplement this technique, for each model we calculated the probability\nassigned to each word in our test set and placed these words into\nlog-spaced buckets based on these probabilities. We calculated the\naverage over all curves in Figure 2 to estimate the fraction of words\ncorrect in each bucket, and collated results over all buckets to get a\n\ufb01nal estimate of word accuracy. We subtract from 1 to produce an\nestimate of word-error rate, and call this measure M-ref. We graph\nthis value versus real word-error rate in Figure 3 for set B.\n\n \n?\n \n?\n \n \n \n@\nA\nB\n \n?\n\fset A\nrank\n0.97\n0.80\n\nlinear\n0.99\n0.94\n\npair\n0.88\n0.68\n\nlinear\n0.92\n0.93\n\nset B\nrank\n0.80\n0.86\n\npair\n0.69\n0.69\n\nPP\nM-ref\n\nTable 2: Correlations of perplexity and measure M-ref with word-\nerror rate\n\nWe considered two methods for estimating the effect of overall lan-\nguage model probabilities on word-error rate: \ufb01rst, we examined\nthe relationship between the absolute language model probability\nassigned to a word and the frequency with which that word occurs\nas an error in speech recognition; and secondly, we examine this\nrelationship except using the relative language model probability of\na word as compared to the probability assigned to the correct word.\nWhen we say a word occurs as an error, we mean that the word oc-\ncurred in the transcription hypothesized by the speech recognizer but\nwas marked as incorrect in word-error rate scoring. It is likely that\nboth absolute and relative probabilities are relevant in determining\nhow frequently a word occurs as an error: if the correct hypothe-\nsis has a very high score, then relative probability is probably more\nimportant; otherwise, absolute probability may play a larger role.\n\nTo estimate the relation between absolute probability and error fre-\nquency, we calculated the language model probability assigned to\neach word in the hypothesis for each utterance in our held-out set.\nWe placed each word deemed incorrect by sclite in logarithmically-\nspaced buckets according to language model probability, to \ufb01nd the\nfrequency of errors in each bucket. To estimate the frequency of\nwords occurring in each bucket in the language model, we evaluated\nthe given language model over all words in the vocabulary over our\nwe evaluated\nheld-out set; i.e., for held-out data\nprobabilities of the form\n1 (cid:20)(cid:23)(cid:21)(cid:22)(cid:21)(cid:22)(cid:21)(cid:22)(cid:20)\n1\nand all words\n. Dividing the errors per bucket by the total number\nof words in each bucket yields an estimate of the probability of a\nword occurring as an error given its language model probability; this\n\nfor all\n\n1\n\n1\n\n(cid:5);(cid:4)$(cid:13)(cid:15)(cid:14)\n\n(cid:20)(cid:22)(cid:21)(cid:23)(cid:21)(cid:22)(cid:21)(cid:22)(cid:20)\n\n(cid:13)(cid:25)(cid:24)(cid:26)(cid:2)\n\n(cid:17)C(cid:18)\n\n%(cid:22)%(cid:22)%\n\nEGF\n\n!D&\n\nr\no\nr\nr\ne\n\ns\na\ng\nn\ni\nr\nr\nu\nc\nc\no\n\nf\no\n\ny\nt\ni\nl\ni\nb\na\nb\no\nr\np\n\n1\n\n0.1\n\n0.01\n\n0.001\n\n0.0001\n\n1e-05\n\n1e-06\n\n1e-07\n\n1e-06\n\n1e-05\n\n0.0001\n\n0.001\nlanguage model probability of word\n\n0.01\n\n0.1\n\n1\n\nFigure 4: Relation between language model probability of a word\nand the frequency with which the word occurs as an error. Each line\nrepresents one of the language models in sets A and B.\n\nr\no\nr\nr\ne\n\ns\na\n\ng\nn\ni\nr\nr\nu\nc\nc\no\n\nf\no\n\ny\nt\ni\nl\ni\nb\na\nb\no\nr\np\n\n10\n\n1\n\n0.1\n\n0.01\n\n0.001\n\n0.0001\n\n1e-05\n\n1e-06\n\n1e-07\n\n1e-08\n\n1e-07\n\n1e-05\n\n0.001\n\n0.1\n\n10\n\n1000\n\n100000 1e+07\n\nlanguage model probability of word relative to correct word\n\nFigure 5: Relation between language model probability of a word\nrelative to the correct word and the frequency with which the word\noccurs as an error\n\nquantity is graphed in Figure 4.1 The different lines correspond to\neach individual model. It is interesting to note the small variation\nbetween the curves for each model, as well as the linearity of the\ncurves as plotted in log-log scale.\n\nTo estimate the relation between relative probability and error fre-\nquency, we used a similar procedure as for absolute probability ex-\ncept that in each step, instead of bucketing by absolute probability\nwe bucket by the ratio between the probability of the given word\nand the \u201ccorrect\u201d word. In order to determine the \u201ccorrect\u201d word,\nwe only consider substitution errors in this analysis. In calculating\nthe language model probability of the correct word, we use the same\nhistory as was used to calculate the language model probability of\nthe given word. Then, using a similar procedure as was described\nabove, we produce the graph displayed in Figure 5. Again, the curves\nare quite linear (in log-log space) and tightly packed, though not as\ntightly as in the previous graph.\n\nWe can use these graphs to create new metrics that approximate\nword-error rate. Since this information is largely orthogonal with\nperplexity, it may be possible to combine the two to achieve a stronger\nmetric. We have yet to explore this avenue.\n\n4. ARTIFICIAL LATTICES\n\nInstead of predicting speech recognition performance by examining\nbasic features of a language model such as perplexity, another ap-\nproach is to attempt to mimic the process of calculating word-error\nrate, except without using a speech recognizer. In this section, we\ndiscuss methods for arti\ufb01cially generating speech recognition lat-\ntices. Word-error rates calculated on these arti\ufb01cial lattices can be\nused to evaluate language models, and we describe a method for\nconstructing lattices such that these arti\ufb01cial word-error rates cor-\nrelate well with word-error rates calculated on genuine lattices. In\naddition, the lattices constructed are very narrow, so that arti\ufb01cial\n\n1It is unclear how to count how often a word occurs in each bucket;\ne.g., during speech recognition, language model probabilities for a word may\nbe estimated multiple times at each position in the utterance with different\nhistories. For the purposes of this calculation, we pretend that a total of\nis the\n\nwords \u201coccur\u201d at each word position in an utterance where\n\nJKI\n\nvocabulary used, and normalize accordingly.\n\n(cid:1)\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:9)\n(cid:1)\n6\n(cid:2)\n(cid:13)\n \n \n \n \nH\n \n \n \n \nH\nI\nJ\n\fsupper\n\nhow\n\nyo\n\nher\n\nthe\n\nyo\n\npick\n\na\n\nyo\n\nFigure 6: An example arti\ufb01cial lattice for the utterance yo yo yo\n\nword-error rates can be calculated quickly.\n\nIn generating lattices, we have made several simplifying assump-\ntions, and have found that the method still works well. First, we\nassume that the correct hypothesis is always in the lattice. Secondly,\nwe assume that all words in a lattice are perfectly time-aligned with\nthe correct hypothesis; i.e., all words in a lattice have the same begin\nand end times as a word in the correct hypothesis \u2014 only substitution\nerrors are considered. One advantage of this assumption is that all\nhypotheses are the same length in words, and an insertion penalty\nhas no effect and can be ignored. Thirdly, we assume that there will\nbe a few words that will be acoustically confusable with each word\nin the correct hypothesis, and that these words will have the same\nacoustic score as the correct word. This is equivalent to only includ-\ning \u201cacoustically confusable\u201d words at each position in the lattice,\nand setting all acoustic scores to zero. With this assumption, the\nlanguage weight becomes irrelevant since all hypotheses have the\nsame acoustic score.\n\ne\nt\na\nr\n\nr\no\nr\nr\ne\n-\nd\nr\no\nw\n\nl\na\ne\nr\n\ne\nt\na\nr\n\nr\no\nr\nr\ne\n-\nd\nr\no\nw\n\nl\na\ne\nr\n\n52\n\n50\n\n48\n\n46\n\n44\n\n42\n\n40\n\n38\n\n36\n\n34\n\n45\n\n44\n\n43\n\n42\n\n41\n\n40\n\n39\n\n38\n\n37\n\n36\n\n15\n\n20\n\n25\n\n20\n\n22\n\n24\n\nset A\n\n30\n\n35\n\nartificial word-error rate\n\nset B\n\n26\n\n28\n\nartificial word-error rate\n\n40\n\n45\n\n50\n\n30\n\n32\n\n34\n\nOur algorithm for generating a lattice on a test-set utterance is as\nfollows. We begin with a lattice that just contains the correct path.\nThe start frames and end frames of each word are unimportant, since\nall words in the lattice will be time-aligned. Then, for each word in\nthe utterance, we randomly generate (according to a distribution to\nwords that occur in the same position (i.e., have the\nbe speci\ufb01ed)\nsame begin and end times). Typically, we have taken\nto be about 9.\nAll acoustic scores are set to zero. In Figure 6, we show an arti\ufb01cial\nlattice for the utterance yo yo yo with\n\n2.\n\nLM(cid:18)\n\nTo generate the words that are \u201cacoustically confusable\u201d with each\nword in the utterance, one possibility is to determine which words are\nacoustically nearby. However, we make the assumption that whether\nwe choose random words or genuinely acoustically confusable words\nwill not affect word-error rate, and use a single probability distribu-\ntion to generate alternatives for all words. One distribution that\n, which\nseems reasonable to use is the unigram distribution\nin the training text. We have\njust re\ufb02ects the frequency of words\nfound empirically that distributions of the form\nproduce lat-\ntices that do well in predicting actual word-error rate, where the value\n0 (cid:21) 5 has worked well in both Broadcast News and Switchboard\n\n(cid:5);N(cid:29)(cid:4)$(cid:13)O(cid:9)\n\n(cid:4)$(cid:13)P(cid:9) Q\n\nexperiments.\n\nLS(cid:18)\n\n9, we generated arti\ufb01cial lattices over our entire\nUsing the value\ntest set. We calculated word-error rates on these arti\ufb01cial lattices for\nall of our models in sets A and B, and in Figure 7 we display a graph\nof arti\ufb01cial word-error rate vs. actual word-error rate over these\nmodels.\nIn Table 3, we display the correlation between arti\ufb01cial\nword-error rate and actual word-error rate. Perplexity is marginally\nbetter on set A, but arti\ufb01cial word-error rate is substantially superior\non set B, the motley mix of models.\n\nFigure 7: Actual word-error rate vs. arti\ufb01cial word-error rate for\nmodels in sets A and B\n\nLT(cid:18)\n\n3 and\n\nGenerating arti\ufb01cial lattices with the values\n0 (cid:21) 5,\nwe compared the correlation between perplexity and arti\ufb01cial word-\nerror rate with actual word-error rate over nine\n-gram models. The\n-gram models were built with varying training data sizes, count\n-gram order. In Table 3, we display the\ncutoffs, smoothing, and\ncorrelations for perplexity and arti\ufb01cial word-error rate; arti\ufb01cial\nword-error rate is superior on this data set.\n\nIn terms of computation, we compare the different metrics through\nthe language model probability evaluations required per word in the\ntest set. Perplexity requires only one language model evaluation per\n\nlinear\nPP\n0.99\nAWER 0.99\n\nBroadcast News\n\nset A\nrank\n0.97\n0.96\n\npair\n0.88\n0.86\n\nlinear\n0.92\n0.96\n\nset B\nrank\n0.80\n0.86\n\npair\n0.69\n0.74\n\nSwitchboard\nlinear\nPP\n0.85\nAWER 0.93\n\nrank\n0.73\n0.83\n\npair\n0.56\n0.67\n\nWe have also performed experiments on the Switchboard task us-\ning lattices generated by the Janus speech recognition system[10].\n\nTable 3: Correlations of perplexity and arti\ufb01cial word-error rate with\nactual word-error rate\n\nL\nL\n(cid:13)\n(cid:5)\nN\nR\n(cid:18)\n \n \n \n \nR\n(cid:18)\n(cid:3)\n(cid:3)\n(cid:3)\n\fLS(cid:18)\n\nword, and is by far the most ef\ufb01cient. For a trigram model, arti\ufb01cial\nL 3 language model evaluations per\nword-error rate requires at most\n9. The time\nword; in practice, the actual value was about 300 for\nrequired to rescore arti\ufb01cial lattices on our 22,000 word held-out set\non a 300 Mhz Pentium II machine ranged from 2 minutes for a trigram\nmodel to 33 minutes for a trigram model with triggers. Rescoring\nactual lattices with a trigram model required about 3600 language\nmodel evaluations per word. The computation time required varied\nfrom 1.6 hours for a trigram model to 18.2 hours for a trigram model\nwith triggers. Thus, calculating arti\ufb01cial word-error rate, while\nsigni\ufb01cantly more expensive than calculating perplexity, is still much\nless expensive than rescoring genuine lattices and the absolute times\ninvolved are quite reasonable.\n\n5. DISCUSSION\nIn this work, we have shown that perplexity can predict word-error\nrate quite well for conventional\n-gram models trained on in-domain\ndata. However, for models of a more disparate nature, perplexity\nis a poorer predictor. We have developed a measure, M-ref, that\nextends perplexity and better predicts word-error rate for complex\nlanguage models. We have also described a technique for generating\narti\ufb01cial lattices such that word-error rates calculated on these lattices\ncorrelate with actual error rates better than perplexity. The error-rate\ncalculation over these lattices is quite inexpensive.\n\nDespite this work, it is still unclear whether perplexity or our novel\nevaluation metrics are effective tools for language modeling re-\nsearchers. Perplexity has been a popular comparison measure his-\ntorically because it allows language model research to develop in\nisolation from speech recognizers, and it has many theoretically el-\negant properties. Unfortunately, this modularization of language\nmodeling is justi\ufb01ed only if our isolated measures can predict ap-\nplication performance accurately enough. While perplexity is an\nindication of performance in the application of text compression, it\nhas been shown to be inadequate in predicting speech recognition\nperformance. For example, one basic criterion of a language model\nevaluation metric is that it can distinguish between language models\nwhose application performances are signi\ufb01cantly different. A word-\nerror rate difference of 0.5% or 1.0% absolute is often considered\nsigni\ufb01cant; if we refer to Figure 1, we \ufb01nd models with essentially\nthe same perplexity that differ by more than 1.0% in error rate. This\nproperty is also true of the novel evaluation metrics that we have\ndescribed. In practice, during language model development for the\nHub 4 evaluations we have discontinued calculating perplexities and\ninstead calculate word-error rates directly to decide whether any\nchanges are useful. Experience has dictated that this is the most\neffective course of action.\n\nWe consider it unlikely that any accurate measure can be developed\nthat, like perplexity, is based only on language model features. This is\nbecause a great many factors affect speech recognition performance:\nthe values of the language weight and insertion penalty; the search\nalgorithm used (search algorithms for long-distance models tend to\nbe less effective); the stage at which the language model is applied\n-best list rescoring); the language\n(decoding, lattice rescoring, or\nmodels used in the other stages; and the interaction of the language\nmodel with the acoustic model. All of these factors signi\ufb01cantly\nimpact recognition performance, and it is unclear how any metric\nthat is blind to these factors could compensate for their effects.\n\nMeasures that imitate the speech-recognition process can abstract\nover many of these issues. For example, in arti\ufb01cial lattice genera-\n\ntion, the search algorithm is not an issue if we assume different search\nalgorithms over arti\ufb01cial lattices cause the same variation in perfor-\nmance as in real lattices. If we have acoustic scores in our arti\ufb01cial\nlattices, then we can optimize language weights over arti\ufb01cial lattices\njust as in real lattices. However, as measures become more complex\nand expensive to compute, calculating word-error rates directly will\nbecome a more attractive alternative.\n\nIn conclusion, existing measures such as perplexity or our novel mea-\nsures are not accurate enough to be effective tools in language model\ndevelopment for speech recognition, and it is unclear how useful it\nis to continue to compare language models for speech recognition\nusing perplexity. While this leaves researchers with the unpleasant\nrequirement that they compare language models only with respect to\nthe same speech recognizer, it does not seem there is a reasonable\nalternative unless more effective measures are developed. There are\ntechniques for making word-error rate computation less expensive,\n-best list rescoring or lattice rescoring with narrow-beam\nsuch as\nlattices, and such techniques are in common use in practice. Indeed,\nto move solely to word-error rate reporting just mirrors the decision\nmade long ago in acoustic modeling, that acoustic models can only\nbe accurately judged in the context of a speech recognition system.\n\nReferences\n\n1. S.C. Martin, J. Liermann, and H. Ney. Adaptive topic-\ndependent language modelling using word-based varigrams.\nIn Proceedings of Eurospeech \u201997, 1997.\n\n2. R. Iyer, M. Ostendorf, and M. Meteer. Analyzing and predict-\ning language model improvements. In Proceedings of the IEEE\nWorkshop on Automatic Speech Recognition and Understand-\ning, 1997.\n\n3. Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-\nnifer C. Lai, and Robert L. Mercer. Class-based n-gram models\nof natural language. Computational Linguistics, 18(4):467\u2013\n479, December 1992.\n\n4. Hermann Ney, Ute Essen, and Reinhard Kneser. On structur-\ning probabilistic dependences in stochastic language modeling.\nComputer, Speech, and Language, 8:1\u201338, 1994.\n\n5. D. Beeferman, A. Berger, and J. Lafferty. A model of lexical\nattraction and repulsion. In Proceedings of the ACL, Madrid,\nSpain, 1997.\n\n6. R. Kuhn and R. De Mori. A cache-based natural language\nmodel for speech reproduction. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 12(6):570\u2013583, 1990.\n\n7. P. Placeway, S. Chen, M. Eskenazi, U. Jain, V. Parikh,\nB. Raj, M. Ravishankar, R. Rosenfeld, K. Seymore, M. Siegler,\nR. Stern, and E. Thayer. The 1996 Hub-4 Sphinx-3 system.\nIn Proceedings of the DARPA Speech Recognition Workshop,\nFebruary 1997.\n\n8. Reinhard Kneser and Hermann Ney.\n\nImproved backing-off\nfor m-gram language modeling. In Proceedings of the IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing, volume 1, pages 181\u2013184, 1995.\n\n9. Slava M. Katz. Estimation of probabilities from sparse data\nfor the language model component of a speech recognizer.\nIEEE Transactions on Acoustics, Speech and Signal Process-\ning, ASSP-35(3):400\u2013401, March 1987.\n\n10. Ivica Rogina and Alex Waibel. The Janus speech recognizer.\n\nIn ARPA SLT Workshop, 1995.\n\n(cid:3)\n(cid:3)\n(cid:3)\n\f",
        "pages_description": [
            "EVALUATION METRICS FOR LANGUAGE MODELS\n\nThe most widely-used evaluation metric for language models in speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, an alternative measure is sought that, like perplexity, is easily calculated but better predicts speech recognition performance.\n\nTwo approaches are investigated: \n1. Utilizing information about language models that perplexity ignores.\n2. Imitating the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices.\n\nOver thirty varied language models were built and tested. It was found that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, the new metrics are superior to perplexity for predicting speech recognition performance. The conclusion is that these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition.\n\n1. INTRODUCTION\n\nIn the literature, two primary metrics are used to estimate the performance of language models in speech recognition systems:\n1. Word-error rate (WER): The number of errors yielded when placed in a speech recognition system.\n2. Perplexity: The inverse of the (geometric) average probability assigned to each word in the test set by the model.\n\nWhile word-error rate is the most popular method for rating speech recognition performance, it is",
            "1.1. Previous Work\n\nThe study by Iyer et al. investigates the prediction of speech recognition performance for language models in the Switchboard domain. The research focuses on trigram models built on varying amounts of in-domain and out-of-domain training data. The findings indicate that perplexity predicts word-error rate well when only in-domain training data is used but performs poorly when out-of-domain data is added. The fraction of trigrams in the test data present in the training data is a better predictor of word-error rate than perplexity. However, extending n-gram coverage to compare other types of models, such as class models or n-gram models of different orders, remains unclear. This measure cannot distinguish between different models trained on the same data.\n\nThe authors also present techniques for building a decision tree to predict the relative performance of two models on each word in a test set. Using this decision tree, they are able to predict with high accuracy the relative performance of pairs of trigram models. While this technique seems promising, the features used to build the tree include lexical information such as part-of-speech information and the phonetic lengths of words. This work aims to investigate what is possible with measures like perplexity that ignore detailed lexical information.\n\n1.2. Methodology\n\nThis research investigates speech recognition performance for the Broadcast News domain. Narrow-beam lattices with the Sphinx-III recognition system were generated using a trigram model built on 30M words of Broadcast News text.",
            "EXTENDING PERPLEXITY\n\nTo understand the relationship between language model probability and word accuracy, we analyze the correlation between word-error rate and log perplexity. The analysis shows that word-error rate is a linear function of perplexity, as depicted in Figure 1 for sets A and B. \n\nFigure 2 illustrates the probability of a word being correct in speech recognition given its language model probability. Each line represents one of the language models in sets A and B. The correlation between different metrics with word-error rate is quantified using the linear correlation coefficient (Pearson's r), the Spearman rank-order correlation coefficient, and Kendall's \u03c4. These measures help in understanding how well the ranks of models correlate and predict the relative performance of pairs of models. For set A, perplexity correlates with word-error rate better than measure M-ref, while for set B, measure M-ref is marginally better.\n\nIn Section 3.1, we model the relation between language model probability and word accuracy by estimating word-error rate from log perplexity. This involves calculating the probability assigned to each word in the test set and placing these words into log-spaced buckets based on these probabilities. The average of the curves in Figure 2 is used to estimate the fraction of words correct in each bucket, which is then subtracted from 1 to produce an estimate of word-error rate, called measure M-ref. Figure 3 shows the word-error rate versus measure M-ref for set B.\n\nSection 3",
            "Correlations of perplexity and measure M-ref with word-error rate\n\nTwo methods were considered for estimating the effect of overall language model probabilities on word-error rate. The first method examined the relationship between the absolute language model probability assigned to a word and the frequency with which that word occurs as an error in speech recognition. The second method examined this relationship using the relative language model probability of a word compared to the probability assigned to the correct word. When a word occurs as an error, it means that the word occurred in the transcription hypothesized by the speech recognizer but was marked as incorrect in word-error rate scoring. It is likely that both absolute and relative probabilities are relevant in determining how frequently a word occurs as an error. If the correct hypothesis has a very high score, then relative probability is probably more important; otherwise, absolute probability may play a larger role.\n\nTo estimate the relation between absolute probability and error frequency, the language model probability assigned to each word in the hypotheses for each utterance in the hold-out set was calculated. Words deemed incorrect by scoring were placed in logarithmically-spaced buckets according to language model probability to find the frequency of errors in each bucket. The given language model over all words in the vocabulary over the hold-out set was evaluated, and the probabilities for the form p(w|w1...wi-1) for all i and all words w were computed. Dividing the errors per bucket by the total number of words in each bucket yields an estimate of the",
            "**Generating Artificial Lattices for Word-Error Rate Calculation**\n\nIn generating lattices, several simplifying assumptions are made to ensure the method remains effective. Firstly, the correct hypothesis is always included in the lattice. Secondly, all words in a lattice are perfectly time-aligned with the correct hypothesis, meaning they share the same begin and end times as the correct words, considering only substitution errors. This assumption implies that all hypotheses are of the same length, and insertion penalties are ignored. Thirdly, a few words are assumed to be acoustically confusable with each word in the correct hypothesis, sharing the same acoustic score.\n\nTo generate artificial lattices, a test-set utterance is taken, and a lattice containing the correct path is created. Words are time-aligned, and for each word, random alternatives are generated, ensuring they share the same begin and end times. Typically, the number of alternatives, denoted as \\( k \\), is around 9. All acoustic scores are set to zero. An example artificial lattice for the utterance \"yo yo yo yo\" with \\( k = 2 \\) is shown.\n\nTo determine acoustically confusable words, a unigram distribution \\( p(w) \\) is used, reflecting word frequencies in the training text. This method assumes that random words or genuinely confusable words do not affect the word-error rate. The distribution \\( p(w) \\) is found effective in predicting actual word-error rates, with a correlation value \\( \\alpha =",
            "### DISCUSSION\n\nIn this work, we have shown that perplexity can predict word-error rate (WER) well for conventional n-gram models trained on in-domain data. However, for models of a more disparate nature, perplexity is a poorer predictor. We have developed a measure, Mr, that extends perplexity and better predicts word-error rate for complex language models. We have also described a technique for generating artificial lattices such that word-error rates calculated on these lattices correlate with actual error rates better than perplexity. The error-rate calculation over these lattices is quite inexpensive.\n\nDespite this work, it is still unclear whether perplexity or our novel evaluation metrics are effective for large language modeling researchers. Perplexity has been a popular comparison measure historically, but it is an indirect measure of speech recognition performance. It is based on language model features, which may not correlate well with actual speech recognition performance due to the complex interaction of various factors such as language weight, insertion penalty, search algorithm, and the integration of language models with acoustic models.\n\nWe have discussed that artificial lattice generation can abstract over many of these issues. For example, in artificial lattice generation, the search algorithm is not an issue if we assume different search algorithms over artificial lattices cause the same variation in performance as in real lattices. If we have acoustic scores for artificial lattices, then we can optimize language weights over artificial lattices just as in real lattices. However, as measures become more complex and expensive to"
        ]
    }
]